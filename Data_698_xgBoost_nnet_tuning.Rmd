---
title: "Data_698_xgBoost_nnet_tuning"
author: "Dan Wigodsky"
date: "May 7, 2019"
output: html_document
---

This file accompanies Data_698_NNet.  This contains visualizations for tuning parameters for xgBoost and for a neural net.   Data_698_NNet creates the neural net model.  The xgBoost models were created in Data_698_regression_and_data_explor.  
  
  
```{r load-packages}
suppressWarnings(suppressMessages(library(plot3D)))
suppressWarnings(suppressMessages(library(showtext)))
suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(kableExtra)))
suppressWarnings(suppressMessages(library(expsmooth)))
suppressWarnings(suppressMessages(library(fastDummies)))
suppressWarnings(suppressMessages(library(gridExtra)))
suppressWarnings(suppressMessages(library(nnet)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(ggforce)))
suppressWarnings(suppressMessages(library(car)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(rms)))
font_add_google(name = "Corben", family = "corben", regular.wt = 400, bold.wt = 700)
  font_add_google(name = "Bree Serif", family = "breeserif", regular.wt = 400, bold.wt = 700)
showtext_auto()
set.seed(563)
``` 


```{r tuning_visualizations,fig.showtext = TRUE,echo=FALSE,fig.height=4,fig.width=4,warning=FALSE,eval=TRUE}
xgBoost_tuning_set<-read.csv('C:/Users/dawig/Desktop/Data698/Maryland/notes/xgBoost_tuning.csv')

head(xgBoost_tuning_set)
ggplot()+geom_point(aes(x=max_depth,y=r_m_s_e),color='#576da8',data=xgBoost_tuning_set)+labs(x='max depth',y='Root Mean Square Error')+ theme(panel.background = element_rect(fill = '#f4f4ef'),panel.grid.major = element_blank(), panel.grid.minor = element_blank(),text = element_text(family = 'corben',color='#249382',size=22))  

ggplot()+geom_point(aes(x=n_threads,y=r_m_s_e),color='#576da8',data=xgBoost_tuning_set)+labs(x='number of threads',y='Root Mean Square Error')+ theme(panel.background = element_rect(fill = '#f4f4ef'),panel.grid.major = element_blank(), panel.grid.minor = element_blank(),text = element_text(family = 'corben',color='#249382',size=22))  

``` 


```{r tuning_visualizations-b,fig.showtext = TRUE,echo=FALSE,fig.height=8,fig.width=8,warning=FALSE,eval=TRUE}

scatter3D(xgBoost_tuning_set$max_depth,xgBoost_tuning_set$eta,xgBoost_tuning_set$r_m_s_e,theta=35,phi=-4,type = "h", ticktype = "detailed",xlab = "max depth", ylab = "eta", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)

xgBoost_tuning_set %>% 
  filter(max_depth==4) %>% 
  filter(eta<.22) %>% 
  filter(eta>.1)->xgBoost_tuning_set
scatter3D(xgBoost_tuning_set$n_rounds,xgBoost_tuning_set$eta,xgBoost_tuning_set$r_m_s_e,theta=55,phi=-4, ticktype = "detailed",xlab = "number of rounds", ylab = "eta", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)
scatter3D(x = 4200, y = .163, z = 8.9523, add = TRUE, colkey = FALSE, 
         pch = 16, cex = 3, col = "#ea5325")

``` 
  
Neural net tuning images follow.  

```{r tuning_visualizations-nnet,fig.showtext = TRUE,echo=FALSE,fig.height=8,fig.width=8,warning=FALSE,eval=TRUE}
nnet_tuning_set<-read.csv('C:/Users/dawig/Desktop/Data698/Maryland/notes/nnet_tuning.csv')

scatter3D(nnet_tuning_set$decay,nnet_tuning_set$MaxNwts,nnet_tuning_set$r_m_s_e,theta=35,phi=-4,type = "h", ticktype = "detailed",xlab = "decay", ylab = "MaxNwts", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)

scatter3D(nnet_tuning_set$decay,nnet_tuning_set$MaxNwts,nnet_tuning_set$r_m_s_e,theta=35,phi=-24,type = "h", ticktype = "detailed",xlab = "decay", ylab = "MaxNwts", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)


scatter3D(nnet_tuning_set$size,nnet_tuning_set$maxit,nnet_tuning_set$r_m_s_e,theta=35,phi=-4,type = "h", ticktype = "detailed",xlab = "size", ylab = "maxit", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)

nnet_tuning_set %>% 
  filter(maxit>5000) %>% 
  filter(r_m_s_e<14)->nnet_tuning_set_b

scatter3D(nnet_tuning_set_b$size,nnet_tuning_set_b$maxit,nnet_tuning_set_b$r_m_s_e,theta=55,phi=-4,type = "h", ticktype = "detailed",xlab = "size", ylab = "maxit", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)

scatter3D(nnet_tuning_set_b$size,nnet_tuning_set_b$maxit,nnet_tuning_set_b$r_m_s_e,theta=90,phi=-4,type = "h", ticktype = "detailed",xlab = "size", ylab = "maxit", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)

nnet_tuning_set %>% 
  filter(maxit>2000)->nnet_tuning_set_b
#9000 iterations work best with some consistancy
scatter3D(nnet_tuning_set_b$size,nnet_tuning_set_b$maxit,nnet_tuning_set_b$r_m_s_e,theta=75,phi=-4,type = "h", ticktype = "detailed",xlab = "size", ylab = "maxit", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)

#sizes of 4,5,6 do best.  4 is the most efficient and possibly best
scatter3D(nnet_tuning_set_b$size,nnet_tuning_set_b$maxit,nnet_tuning_set_b$r_m_s_e,theta=0,phi=-15,type = "h", ticktype = "detailed",xlab = "size", ylab = "maxit", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)


#For MaxNwts, 10 looks best.  For decay, .2 looks best.  This is rather unclear
scatter3D(nnet_tuning_set_b$decay,nnet_tuning_set_b$MaxNwts,nnet_tuning_set_b$r_m_s_e,theta=90,phi=-15,type = "h", ticktype = "detailed",xlab = "decay", ylab = "MaxNwts", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)

scatter3D(nnet_tuning_set_b$decay,nnet_tuning_set_b$MaxNwts,nnet_tuning_set_b$r_m_s_e,theta=0,phi=-15,type = "h", ticktype = "detailed",xlab = "decay", ylab = "MaxNwts", zlab = "root mean square error",cex.axis=1.4,cex.lab=2,cex=2.5)

ggplot()+geom_point(aes(x=decay,y=r_m_s_e),color='#576da8',data=nnet_tuning_set)+labs(x='decay',y='Root Mean Square Error')+ theme(panel.background = element_rect(fill = '#f4f4ef'),panel.grid.major = element_blank(), panel.grid.minor = element_blank(),text = element_text(family = 'corben',color='#249382',size=22))  

ggplot()+geom_point(aes(x=MaxNwts,y=r_m_s_e),color='#576da8',data=nnet_tuning_set)+labs(x='MaxNwts',y='Root Mean Square Error')+ theme(panel.background = element_rect(fill = '#f4f4ef'),panel.grid.major = element_blank(), panel.grid.minor = element_blank(),text = element_text(family = 'corben',color='#249382',size=22))  

```  
  
To choose the best values for parameters, we run regressions  


```{r nnet-param-regression,fig.showtext = TRUE,echo=FALSE,fig.height=8,fig.width=8,warning=FALSE,eval=TRUE}
nnet_tuning_set_dummies<-nnet_tuning_set
nnet_tuning_set_dummies$size<-as.factor( nnet_tuning_set_dummies$size )
nnet_tuning_set_dummies$decay<-as.factor(nnet_tuning_set_dummies$decay )
nnet_tuning_set_dummies$MaxNwts<-as.factor(nnet_tuning_set_dummies$MaxNwts)
nnet_tuning_set_dummies<-dummy_cols(nnet_tuning_set_dummies)

nnet_tuning_set_dummies<-nnet_tuning_set_dummies[,c(3,5,6,8,9,10,11,12,13,14,15,16,17,18,20,21,22,23,24,25)]

best_level_model<-lm(r_m_s_e~.,data=nnet_tuning_set_dummies)
summary(best_level_model)
```

The best parameters are: maxit=9000 (from graphs), decay=.1,size=7,MaxNwts=10.   7 was the maximum size attempted in our first analysis.  As a result, we ran longer models, which performed well with many layers. In later runs,  22 layers worked well. 
  
  